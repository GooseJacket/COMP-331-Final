{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "12uGNvQADxlbLJ713agP3EmMNVEQVlJi5",
      "authorship_tag": "ABX9TyOdKmV7LLPihQFp6zBgPYhs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GooseJacket/COMP-331-Final/blob/Grammar/Angela_Merrill_Final_Grammar_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COMP 331 Final (Angela Merrill) - Grammar Corrector!\n",
        "\n",
        "Guided by [this Bert Classification Tutorial](https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX) and [the given PA3 Text Generation Tutorial](https://colab.research.google.com/drive/1csEkXfVMAtjVQ7FGmUthlcnkclgELaAp?usp=sharing)\n",
        "\n",
        "Data from [This Kaggle](https://www.kaggle.com/datasets/satishgunjal/grammar-correction)"
      ],
      "metadata": {
        "id": "mPKk7mDoSzsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "V5kAf-peB0ma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "collapsed": true,
        "id": "gv_dExEFSt2L"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import random\n",
        "from random import shuffle\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n"
      ],
      "metadata": {
        "id": "h4n53_MeTKLU"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to GPU - pulled from Bert Text Classification Tutorial\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67OPkitpn1PR",
        "outputId": "fa5d6958-1828-42a0-8606-c26ed6438860"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from csv\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Grammar Correction.csv\")\n",
        "\n",
        "# Should be 2018 bad-good sentence pairs!\n",
        "print(\"Data Size:\", df.shape[0])\n",
        "\n",
        "df.head()\n",
        "\n",
        "# Shuffle because it is sorted by type of grammar mistake\n",
        "df = df.sample(len(df))\n",
        "\n",
        "# get the good and bad lists\n",
        "bad = [i for i in df[\"Ungrammatical Statement\"].values]\n",
        "good = [i for i in df[\"Standard English\"].values]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4Oqhv6WS9kX",
        "outputId": "2077afe8-cd23-4a27-cbd3-121e4937ebf5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Size: 2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Train Text Classification\n",
        "\n",
        "Mainly aided by [this tutorial!](https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX)"
      ],
      "metadata": {
        "id": "Jc1cWeO96gEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "W3PwL2H1CMAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type_dict = {}\n",
        "\n",
        "for i in df[\"Error Type\"]:\n",
        "  if i in type_dict:\n",
        "    type_dict[i] += 1\n",
        "  else:\n",
        "    type_dict[i] = 1\n",
        "\n",
        "type_dict\n",
        "\n",
        "# 36 x 40"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9R8bIkIG6lw7",
        "outputId": "52b743b4-681a-454a-ab37-0cd7fcfe4d0b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Mixed Conditionals': 49,\n",
              " 'Clich√©s': 48,\n",
              " 'Sentence Structure Errors': 103,\n",
              " 'Conjunction Misuse': 49,\n",
              " 'Inappropriate Register': 49,\n",
              " 'Preposition Usage': 95,\n",
              " 'Pronoun Errors': 47,\n",
              " 'Article Usage': 100,\n",
              " 'Run-on Sentences': 40,\n",
              " 'Mixed Metaphors/Idioms': 50,\n",
              " 'Sentence Fragments': 40,\n",
              " 'Quantifier Errors': 48,\n",
              " 'Gerund and Participle Errors': 50,\n",
              " 'Faulty Comparisons': 49,\n",
              " 'Capitalization Errors': 40,\n",
              " 'Incorrect Auxiliaries': 50,\n",
              " 'Relative Clause Errors': 51,\n",
              " 'Verb Tense Errors': 100,\n",
              " 'Punctuation Errors': 60,\n",
              " 'Slang, Jargon, and Colloquialisms': 50,\n",
              " 'Lack of Parallelism in Lists or Series': 50,\n",
              " 'Infinitive Errors': 49,\n",
              " 'Parallelism Errors': 49,\n",
              " 'Contractions Errors': 49,\n",
              " 'Subject-Verb Agreement': 100,\n",
              " 'Agreement in Comparative and Superlative Forms': 49,\n",
              " 'Redundancy/Repetition': 20,\n",
              " 'Negation Errors': 50,\n",
              " 'Word Choice/Usage': 40,\n",
              " 'Abbreviation Errors': 50,\n",
              " 'Ambiguity': 50,\n",
              " 'Modifiers Misplacement': 46,\n",
              " 'Passive Voice Overuse': 49,\n",
              " 'Ellipsis Errors': 49,\n",
              " 'Tautology': 50,\n",
              " 'Spelling Mistakes': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The tutorial had the text and labels as ndarrays, so I use np.hstack to keep that consistant\n",
        "class_train_texts = np.hstack([bad, good])\n",
        "class_train_lens = [len(str(i).split(\" \")) for i in class_train_texts]\n",
        "class_train_labels = np.hstack([[0 for i in range(len(bad))], [1 for i in range(len(good))]])\n",
        "\n",
        "# make sure it's working\n",
        "print(class_train_texts[0])  # should have grammar error\n",
        "print(class_train_labels[0])  # should be 0\n",
        "print(class_train_texts[-1])  # should be good grammar\n",
        "print(class_train_labels[-1])  # should be 1\n",
        "\n",
        "print(\"\\nMax Length =\", max(class_train_lens))  # get max length!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mts1pYD-7j47",
        "outputId": "bde551e7-25a9-40a3-c7a2-d8f77b76fbb1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If I won the lottery, I could have traveled the world.\n",
            "0\n",
            "You should have been more careful while handling the equipment.\n",
            "1\n",
            "\n",
            "Max Length = 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "2M8rPFgyAgKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch got messed up!\n",
        "# !pip uninstall torch\n",
        "# !pip install torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fAugLapEgox5"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "class_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XjXVVhrbfoGU",
        "outputId": "bb36f8e1-362c-47b3-a747-715a529d2adf"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(class_train_lens) * 2  # to account for punctuation and larger things later\n",
        "\n",
        "def tokenizeTexts(texts, labels):\n",
        "  # Pulled straight from the BERT Text Classification Tutorial\n",
        "  # texts (list of sentences) --> ids, attention_masks, labels\n",
        "\n",
        "  ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in texts:\n",
        "    encoded_dict = class_tokenizer.encode_plus(\n",
        "      sent,                          # sentence\n",
        "      add_special_tokens = True,     # add [CLS] and [SEP]\n",
        "      max_length = max_len,          # used to pad to same size\n",
        "      padding = \"max_length\",        # Had to change this from the tutorial!\n",
        "      return_attention_mask = True,  # Get attention masks to ignore padding\n",
        "      return_tensors = 'pt',          # Use pytorch\n",
        "      truncation = True\n",
        "    )\n",
        "\n",
        "    # Log tokenized sentence and its attention mask\n",
        "    ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Lists --> tensors.\n",
        "  ids = torch.cat(ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  return ids, attention_masks, labels\n",
        "\n",
        "class_train_ids, class_train_attention_masks, class_train_labels = tokenizeTexts(class_train_texts, class_train_labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', class_train_texts[0])\n",
        "print('Token IDs:', class_train_ids[0])\n",
        "print('Attentions:', class_train_attention_masks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Do5bDQiMn2",
        "outputId": "f79aa314-30fc-41f8-bfd3-ed5c03f3197f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  If I won the lottery, I could have traveled the world.\n",
            "Token IDs: tensor([  101,  1409,   146,  1281,  1103, 23366,   117,   146,  1180,  1138,\n",
            "         5505,  1103,  1362,   119,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n",
            "Attentions: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets/loaders"
      ],
      "metadata": {
        "id": "W1GjL49RChlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "class_train_dataset = TensorDataset(\n",
        "    class_train_ids,\n",
        "    class_train_attention_masks,\n",
        "    class_train_labels)\n",
        "\n",
        "class_train_batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order.\n",
        "class_train_dataloader = DataLoader(\n",
        "    class_train_dataset,  # The training samples.\n",
        "    batch_size = class_train_batch_size, # Trains with this batch size.\n",
        "    shuffle = True\n",
        ")"
      ],
      "metadata": {
        "id": "EMpQtC3vmZ-y"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Model"
      ],
      "metadata": {
        "id": "JTXysBUVCws8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BertForSequenceClassification\n",
        "class_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",          # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2,               # Grammatical or Ungrammatical\n",
        "    output_attentions = False,    # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "class_model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aBJl9pXxmo96",
        "outputId": "27afc73b-d853-47c2-b253-65335ae0d82c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Huggingface's AdamW seems to be broken so I replaced it with torch's\n",
        "class_train_opt = torch.optim.AdamW(class_model.parameters(),\n",
        "                  lr = 5e-5, # Learning Rate\n",
        "                  eps = 1e-8 # Epsilon?\n",
        "                )\n",
        "\n",
        "class_train_epochs = 2\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "class_train_scheduler = get_linear_schedule_with_warmup(class_train_opt,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = len(class_train_dataloader) * class_train_epochs)"
      ],
      "metadata": {
        "id": "DK1vbEtVnlRM"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "tyEZFJ_Movpd"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "metadata": {
        "id": "YMqlg_1xpMDL"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train!"
      ],
      "metadata": {
        "id": "V1wgalFbmpqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulled from Bert Text Classification Tutorial!\n",
        "# I got rid of the validation to increase the training data.\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "class_train_training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "class_train_total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, class_train_epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, class_train_epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    class_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(class_train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(class_train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        class_model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward`\n",
        "        # function and pass down the arguments. The `forward` function is\n",
        "        # documented here:\n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        result = class_model(b_input_ids,\n",
        "                       token_type_ids=None,\n",
        "                       attention_mask=b_input_mask,\n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(class_model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        class_train_opt.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        class_train_scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(class_train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    class_train_training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-class_train_total_t0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwH-Z75KpNjL",
        "outputId": "5c4c39c5-7bc3-4055-c595-475611ca44e4",
        "collapsed": true
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    253.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    253.    Elapsed: 0:00:13.\n",
            "  Batch   120  of    253.    Elapsed: 0:00:20.\n",
            "  Batch   160  of    253.    Elapsed: 0:00:27.\n",
            "  Batch   200  of    253.    Elapsed: 0:00:33.\n",
            "  Batch   240  of    253.    Elapsed: 0:00:40.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0:00:43\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    253.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    253.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    253.    Elapsed: 0:00:20.\n",
            "  Batch   160  of    253.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    253.    Elapsed: 0:00:32.\n",
            "  Batch   240  of    253.    Elapsed: 0:00:39.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0:00:41\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:24 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Function\n",
        "input: text paragragh\n",
        "\n",
        "output: list of guesses as un/grammatical"
      ],
      "metadata": {
        "id": "qI8dDgeTtbwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):  # text = paragrah\n",
        "  # turn paragraph into list of sentences\n",
        "  splits = [\"‚Ä¶\", \".\", \"?\", \"!\", \";\", \"\\n\"]\n",
        "  texts = text\n",
        "  for s in splits:\n",
        "    texts = re.sub(re.escape(s), s+\"SPLITHERE\", texts)\n",
        "  texts = [i for i in texts.split(\"SPLITHERE\") if i not in [\"\", \"\\n\"]]\n",
        "\n",
        "  # we need fake labels!\n",
        "  labels = [0 for i in texts]\n",
        "\n",
        "  # set up data into dataloader\n",
        "  ids, attention_masks, labels = tokenizeTexts(texts, labels)\n",
        "  predict_dataset = TensorDataset(ids, attention_masks, labels)\n",
        "  predict_dataloader = DataLoader(\n",
        "              predict_dataset,\n",
        "              sampler = RandomSampler(predict_dataset), # Shuffle the data\n",
        "              batch_size = len(texts) # Go over entire paragraph\n",
        "          )\n",
        "\n",
        "  for batch in predict_dataloader:\n",
        "    class_model.eval()\n",
        "    with torch.no_grad():\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "\n",
        "      # Run the model over the data\n",
        "      result = class_model(b_input_ids,\n",
        "                      token_type_ids=None,\n",
        "                      attention_mask=b_input_mask,\n",
        "                      labels=b_labels,\n",
        "                      return_dict=True)\n",
        "\n",
        "      # Get the predictions\n",
        "      logits = result.logits.detach().cpu().numpy()\n",
        "\n",
        "  # Argmax to get the actual predictions\n",
        "  guesses = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "  return texts, guesses\n",
        "\n",
        "\n",
        "predict_texts = [\n",
        "    \"I has a nightmare last night.\",                        # 0\n",
        "    \"I went to the store but I forgets my shopping lsit!\",  # 0\n",
        "    \"It was unfortunately embarrassing!\",                   # 1\n",
        "    \"But whatever I can deal.\"\n",
        "]\n",
        "predict(\" \".join(predict_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vDpEUfOQs7r0",
        "outputId": "24869893-4ac7-4cd3-cf6e-f3a6bdf3803c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['I has a nightmare last night.',\n",
              "  ' I went to the store but I forgets my shopping lsit!',\n",
              "  ' It was unfortunately embarrassing!',\n",
              "  ' But whatever I can deal.'],\n",
              " array([0, 1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Text Generation\n",
        "\n",
        "Mainly aided by the PA3 Tutorial!"
      ],
      "metadata": {
        "id": "vnhiNvDy96Bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "d9-izeHKHbJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)\n",
        "\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch.optim as optim\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lYjj85kEKVb",
        "outputId": "4c8588f3-5eff-47f7-8f9e-c757fce97025"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "cGpgxwEKBwnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_train_texts = np.dstack([bad, good])[0]\n",
        "\n",
        "gen_train_texts = [\" <Bad> \" + i[0] + \" <Fixed> \" + i[1] for i in gen_train_texts]\n",
        "gen_train_lens = [len(str(i).split(\" \")) for i in gen_train_texts]\n",
        "gen_max_len = int(sum(gen_train_lens)/len(gen_train_lens))\n",
        "\n",
        "print(gen_train_texts[0])\n",
        "print(gen_train_texts[-1])\n",
        "print(\"\\Max Length =\", gen_max_len)  # get average length!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ZRFAQ8_Urw",
        "outputId": "d535bd1b-5e0c-4552-c575-e20f10430e3c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <Bad> If I won the lottery, I could have traveled the world. <Fixed> If I won the lottery, I could travel the world.\n",
            " <Bad> You shoulda been more careful while handling the equipment. <Fixed> You should have been more careful while handling the equipment.\n",
            "\\Max Length = 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:9: SyntaxWarning: invalid escape sequence '\\M'\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\M'\n",
            "/tmp/ipython-input-228450688.py:9: SyntaxWarning: invalid escape sequence '\\M'\n",
            "  print(\"\\Max Length =\", gen_max_len)  # get average length!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "3kK1Oj1nD-ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = {\n",
        "    \"bad\": \"<Bad>\",\n",
        "    \"good\": \"<Fixed>\"\n",
        "}\n",
        "\n",
        "# Load the GPT tokenizer.\n",
        "gen_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>', padding_side='left', extra_special_tokens=special_tokens)\n",
        "# padding from the left because it's a decoder-only architecture"
      ],
      "metadata": {
        "id": "qnYhTM8ZEAc2"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]"
      ],
      "metadata": {
        "id": "jTb7cHsMEY72"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets/loaders"
      ],
      "metadata": {
        "id": "qZ9-FugOIBtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_train_dataset = GPT2Dataset(gen_train_texts, gen_tokenizer, max_length=gen_max_len * 3)\n",
        "\n",
        "gen_train_batch_size = 16\n",
        "\n",
        "gen_train_dataloader = DataLoader(\n",
        "            gen_train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(gen_train_dataset), # Select batches randomly\n",
        "            batch_size = gen_train_batch_size # Trains with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "jUgt382HEbBb"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set up Model"
      ],
      "metadata": {
        "id": "gFOiiSGuID-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# instantiate the model\n",
        "gen_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "# Fix model size to account for special tokens\n",
        "gen_model.resize_token_embeddings(len(gen_tokenizer))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "gen_model.cuda()\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "gen_train_epochs = 6\n",
        "\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "\n",
        "# this produces sample output every N steps\n",
        "sample_every = 40\n",
        "\n",
        "gen_train_opt = optim.AdamW(gen_model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                  eps = epsilon\n",
        "                )\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(gen_train_dataloader) * gen_train_epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "# This changes the learning rate as the training loop progresses\n",
        "gen_train_scheduler = get_linear_schedule_with_warmup(gen_train_opt,\n",
        "                                            num_warmup_steps = warmup_steps,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "metadata": {
        "id": "HWoTLnQ-E81I"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train!"
      ],
      "metadata": {
        "id": "3S3qFrzhIFQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_train_total_t0 = time.time()\n",
        "\n",
        "gen_train_training_stats = []\n",
        "\n",
        "# Push model to device (cuda)\n",
        "gen_model = gen_model.to(device)\n",
        "\n",
        "for epoch_i in range(0, gen_train_epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, gen_train_epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    gen_model.train()\n",
        "\n",
        "    for step, batch in enumerate(gen_train_dataloader):\n",
        "\n",
        "        # b_input_ids and b_labels are both set to the input sequences of the batch\n",
        "        # when we pass them to the model, the model will predict one token at a time\n",
        "        # till the end of the sequence\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        gen_model.zero_grad()\n",
        "\n",
        "        outputs = gen_model(  b_input_ids,          # batch input token IDs\n",
        "                          labels=b_labels,      # batch input token IDs\n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample (for human evaluation) every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(gen_train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "            gen_model.eval()\n",
        "\n",
        "            sample_outputs = gen_model.generate(\n",
        "                                    b_input_ids,    # Use a batch of input IDs for generation\n",
        "                                    attention_mask=b_masks, # Pass the attention mask\n",
        "                                    pad_token_id=gen_tokenizer.pad_token_id, # Set the padding token ID as its ID in our tokenizer, otherwise there will be a warning\n",
        "                                    do_sample=True,\n",
        "                                    top_k=100, #ANGELA: changed to 100 to possibly add randomness!\n",
        "                                    max_new_tokens = 200,\n",
        "                                    top_p=0.95,\n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, gen_tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "\n",
        "            gen_model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        gen_train_opt.step()\n",
        "\n",
        "        # Update learning rate based on completed optimizer steps\n",
        "        gen_train_scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(gen_train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    gen_train_training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-gen_train_total_t0)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-JgFBcKFKIi",
        "outputId": "cf8a1128-b521-4bfc-d824-b772de28cd52",
        "collapsed": true
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    127. Loss: 4.121891498565674.   Elapsed: 0:00:12.\n",
            "0:   If you eat too much, you might have felt sick.  If you eat too much, you might feel sick.The Internet Archive is helping kids solve puzzles over grammar and grammar as students help solve simple grammar puzzles.\n",
            "\n",
            "For the teacher at an elementary school in New York that teaches students how to solve math equations using grammar by using computer training, there was no need to bring a new computer to solve an old problem.\n",
            "1:   The company wants to ensure they're customers are satisfied.  The company wants to ensure their customers are satisfied.What if we knew how powerful computer technology does not fix our problem? If you have an old computer, or you couldn't repair it, how likely would you be to have it repaired?\n",
            "2:   The job requires strong analytical skills, attention to detail, and being a good team player.  The job requires strong analytical skills, attention to detail, and good team player abilities.Image caption The first teacher told Mr Khan that when he was asked, he was confident his teacher was going to teach the students. His confidence was not really strong at the time\n",
            "\n",
            "\n",
            "One of 12 teachers at a school in Pakistan is in charge of a new science class and says their teacher is going to teach the students about a new topic.\n",
            "\n",
            "That's no good. They should have known better. The teacher has no confidence. êöúÔøΩ. What she might have asked him was more than a question. ÔøΩÔøΩ ÔøΩ You're using the new language. He should have known better. He can't.\n",
            "3:   The UN has declared a new initiative for SDGs.  The United Nations has declared a new initiative for Sustainable Development Goals.LONDON-- The new administration's new budget for the economy was last year's highest ever, more than the budget for the US, but it's starting to raise some questions about what it would do.\n",
            "\n",
            "The economy has been less than a month full. The administration announced on Monday that it would cut budget for this year by an unspecified amount. The government would start raising its forecast for a new quarter by 5 billion this year, and this would put it at a new level of 2.5 billion by the end of 2013.\n",
            "4:   The waiter serve the food to the customers.  The waiter served the food to the customers.\"She has been to my house. I went for a change and it was delicious. She was amazing.\"\n",
            "5:   61. The city is known for its historic architecture and buetiful parks.  61. The city is known for its historic architecture and beautiful parks.I've had fun playing with my friends on Steam for more than a week. I've been playing a lot lately and playing through it. I was learning more about the game and had fun playing through it, and tried to watch a lot of its new features. ÔøΩ Let me know you're playing with your friends while they're doing it.\n",
            "6:   The man which bought the car was happy.  The man who bought the car was happy.Overview of features: A high-performance car and an efficient and efficient design. The car looks great, but is not the perfect fit. In my opinion the car is very expensive. The car should come to you to improve your driving skills, but you will need to drive at the pace you want to go more than an hour a day.\n",
            "7:   The cat catch the mouse yesterday.  The cat caught the mouse yesterday.An unexpected change has been announced for the new iPhone 6. The 7. The cat arrived at the store today, but it was promptly released, the phone's return is soon after yesterday. Now that we're sure they're going for a new phone, but have been waiting to see the new battery and charge it. ÔøΩÔøΩ The new battery is a new phone.\n",
            "8:   How are you doing today?  How are you doing today?An early Saturday morning update for students at the school of architecture went viral yesterday. In the evening of learning to perform tasks for an evening class in the campus hall, students went to the building. This morning, they got on the hall watch party.\n",
            "9:   He wanted to go to the park but his mother said no.  He wanted to go to the park, but his mother said no.(Swanhong-chincha's trip to college is a special experience. The trip can't take more than ten minutes.  Ë°ú „ÅÑ„Çà‰Ωç„Åó‰∏ÄÊ£ãËΩº ÔøΩÁî∞„Åç„Å¶„Çâ„Åü The trip was so well worth it. I've been on the last day of the trip since ‰∏≠Êñº minutes.\n",
            "10:   The interview was conducted by the journalist with a famous celebrity.  The interview with a famous celebrity was conducted by the journalist.The interview was conducted by the owner of an award-winning music festival in Singapore. The singer was asked, in the interview, about his thoughts, his previous engagement with a popular singer.\n",
            "11:   The concert we went to it last night was amazing.  The concert that we went to last night was amazing.A few hours ago our team of engineers released some new analysis for Unity 5.00 using the latest features that were available at Microsoft. If you're not a developer and you're interested, or have a new tool, please take a look at this new work\n",
            "12:   The table was set by Emily for dinner.  Emily set the table for dinner.In Japan of course she is busy answering your question about the changes she's making in her new job.\n",
            "13:   To efficiently use energy, turn off lights when not in use.  To use energy efficiently, turn off lights when not in use.What happened in the new year? I want to answer the questions asked in this last post. My wife, I've been so busy this year since moving to the house. ‰πù The changes in your office. The change in your new building last year is difficult for me. ‰∏äËØù I don't want to answer your questions here. ‰Ωï ËØù ËØù ‰∏çÂèù ÈóÆ‰πù I understand. ËØù Êúâ You were busy last year with new projects. ‰πù This year is hard. ÔøΩ They're now so busy. ÔøΩ you're moving here. ÔøΩ she didn't take a long break.\n",
            "14:   The soccer player was injure during the game.  The soccer player was injured during the game.What you need to remember about the new movie The new movie is not much different. You'll have a lot more of things to go with it than what you're familiar with, so don't have any bad experience. This movie revolves around a very early hour of shooting where I played soccer. The movie takes place five minutes, in a beautiful city.\n",
            "15:   It's color is fading after multiple washes.  Its color is fading after multiple washes.Today's piece is a unique exercise in our modern class of analysis of the ways of productivity and productivity skills that students are exposed to new learning, but without much more than a few more information on their learning.\n",
            "  Batch    80  of    127. Loss: 1.0810285806655884.   Elapsed: 0:00:30.\n",
            "0:   The movie is about a detective solving a mystery based on a true story.  The movie, based on a true story, is about a detective solving a mystery.In what way can you answer these questions?  The answer is simple: If I can't solve the puzzle by doing all my homework, then I can't answer your questions.\n",
            "1:   The teacher asked the students to submit they're assignments by tomorrow.  The teacher asked the students to submit their assignments by tomorrow.0 SHARES Facebook Twitter 0 SHARES Facebook Twitter Share Tweet\n",
            "\n",
            "\n",
            "As you can see in this article, this article was written by one of my favorite authors, Marko, and then from a variety of other writers, I decided to write it in a more complete and more complete way.  If you like to see me in complete and more coherent works, than here, you can't help me.\n",
            "2:   She is waiting for a bus at a bus stop.  She is waiting for the bus at the bus stop.For two days today we're taking the test on our new laptop.  She is waiting at the bus stop.\n",
            "3:   The team is focusing on R&D for the proj.  The team is focusing on research and development for the project.' The team is working on creating a new project.\n",
            "4:   The artist was draw a portrait of the model.  The artist was drawing a portrait of the model.Please report this issue to admin.\n",
            "5:   I met him outside the house.  I met him outside of the house.Welcome to our website.  I am a player with a unique skill set, and an interesting experience.\n",
            "6:   She advised to always think positively.  She advised to think always positively.Please turn off the lights and go outside.  She advised herself to turn off the lights.\n",
            "7:   Some of the furniture in this store is made of high-quality wood.  Some furniture in this store is made of high-quality wood.(CNN) -- The news organization is launching a new user-friendly report system for help with user development.  Some of the furniture in this store is made of high-quality wood.\n",
            "8:   The package was delivered to the wrong address by the courier by mistake.  The package was delivered by the courier to the wrong address by mistake.0 Shares\n",
            "\n",
            "\n",
            "\"I must answer now for being late to the deadline.  I must answer now for being late to the deadline.\n",
            "9:   She said that she was going to the store but she never came back.  She said that she was going to the store, but she never came back.' She said that she was going to see her parents.\n",
            "10:   He promised to immediately call when he arrived.  He promised to call immediately when he arrived.Budgeting an extra $5.5 million in revenue from a pre-tax return is not an unusual budget move.  The budget has more than doubled in the four months since it was announced.\n",
            "11:   She's hair is long and curly.  Her hair is long and curly.A famous and expensive pair of shoes were bought by the great-granddaughter of a very famous designer at the designer's birthday party.  She's beautiful and expensive shoes were bought at a designer's birthday party.\n",
            "12:   42. The baby was crying beacuse it was hungry.  42. The baby was crying because it was hungry.‚Äô 48 hours ago.‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚ÄôÔøΩ\n",
            "13:   John's hobbies include playing soccer, cooking delicious meals, and to travel the world.  John's hobbies include playing soccer, cooking delicious meals, and traveling the world.The researchers found a unique combination of resources in their study that they could use to do more research.  John's work in the field was focused on multiple abilities, making it easier for researchers to use it.\n",
            "14:   The H2O lvl in the tank is getting low.  The water level in the tank is getting low.It is important to be honest with you because we need real-time communication.  The water levels in the tank are getting low because the atmosphere is becoming too heavy.\n",
            "15:   The invitation was accepted by Lisa to attend the event.  Lisa accepted the invitation to attend the event.A report by the Environmental Protection Agency (EPA) in a report on air pollution last year has been released by the National Environmental Law Enforcement Administration that it has been called into question.  The report was released last year by the EPA, and a comprehensive analysis has been completed.\n",
            "  Batch   120  of    127. Loss: 0.7577281594276428.   Elapsed: 0:00:49.\n",
            "0:   I love coffee... only drink it in the morning.  I love coffee, but I only drink it in the morning.Product was delivered from a friend.  I never received my product from my friend.\n",
            "1:   She is the most more experienced candidate for the job.  She is the more experienced candidate for the job.My favorite athlete in the gym is Coach-Dunker Randy Couture.  She is the more experienced candidate for the job.\n",
            "2:   please submit your application to the department of human resources.  Please submit your application to the Department of Human Resources.We're going to the gym tonight.  Please submit your application to the Department of Human Resources.\n",
            "3:   The game challenges players to solve puzzles, explore new worlds, and to defeat powerful enemies.  The game challenges players to solve puzzles, explore new worlds, and defeat powerful enemies.To submit new jobs, please submit a new job.  Please submit the new job at the right job search.\n",
            "4:   The exam was most easier than I thought.  The exam was easier than I thought.Famous celebrity chef has won a gold medal in the Grand Prix series.  Famous celebrity chef has won a gold medal in the Grand Prix Series.\n",
            "5:   I need to buy a new pair of a shoes.  I need to buy a new pair of shoes.Can you buy one?\n",
            "\n",
            "Can you buy one?\n",
            "6:   She putted the flowers in a vase on the table.  She put the flowers in a vase on the table.‚Äô She put the flowers in the vase. ‚Äô‚Äô She put them in the vase.\n",
            "7:   I need to talk with him about the project.  I need to talk to him about the project.Teacher of the house has two children in the house.  I need to talk with him about the project.\n",
            "8:   The bike which I ride to work is green.  The bike that I ride to work is green.Untitled a guest Mar 22, 2017 19. Her favorite is bike.  Her favorite is her favorite.\n",
            "9:   He is interested on learning Spanish.  He is interested in learning Spanish.‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô‚Äô\n",
            "10:   46. I can't believe how fast my daughter is groing up.  46. I can't believe how fast my daughter is growing up.‚Äô The farmer's field is growing crops in the field.  46. I can't believe how fast her growing crops are growing.\n",
            "11:   28. I gotta get this report done by tomorrow.  28. I have to finish this report by tomorrow.I need this information from you.  28. I need this information from you.\n",
            "12:   John needs to clean his room, the car, and organizing his desk.  John needs to clean his room, the car, and organize his desk.‚Äô John needs to clean his room, the car, and organize his desk.\n",
            "13:   Many of the plants in this garden are native to the region.  Many plants in this garden are native to the region.Bachelor candidate is preparing a strong candidate for a job interview.  Many plants in this garden are native to this region.\n",
            "14:   The concert will be held in the park.  The concert will be held at the park.Famed for her stunning photos she posed in her photos on Instagram.  She posed in her photos on her Instagram.\n",
            "15:   The driver parked the car in it's designated spot.  The driver parked the car in its designated spot.Catalog of items was broken due to the impact of the traffic on the car.\n",
            "\n",
            "  Average training loss: 3.51\n",
            "  Training epoch took: 0:01:00\n",
            "\n",
            "======== Epoch 2 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    127. Loss: 0.5592668056488037.   Elapsed: 0:00:12.\n",
            "0:   To safely store the chemicals, use airtight containers.  To store the chemicals safely, use airtight containers.Can you find a safe and affordable way to store them?\n",
            "\n",
            "\n",
            "Can you find a safe and affordable way to store them?\n",
            "1:   The pizza is deliver to the wrong address.  The pizza was delivered to the wrong address.I need to delete my file, but this package is still present.  I need to delete my files.\n",
            "2:   She make a cake for her friend's birthday.  She makes a cake for her friend's birthday.Furniture can be bought for the same price as a shirt.\n",
            "3:   39. You can catch more flies with honey than with vinegar, but the grass is always greener on the other side.  39. You can catch more flies with honey than with vinegar.A woman has found a recipe for a new recipe for a new recipe for an ice cream.  39. You can catch more flies with honey than with salt.\n",
            "4:   The bicycle is more cheaper than the car.  The bicycle is cheaper than the car.‚Äô The bike is more affordable than the car.\n",
            "5:   John's favorite subjects are history, math and science.  John's favorite subjects are history, math, and science.To view this recipe, put the cookies.\n",
            "6:   Apples are more better than oranges.  Apples are better than oranges.You can't judge a book by its cover by its cover; but a great book is a beautiful work.  You can't judge a book by its cover; but a beautiful work is a beautiful work.\n",
            "7:   He is a member of a chess club.  He is a member of the chess club.Abstract We are looking for a doctor to perform surgery on a patient. The patient has surgery.\n",
            "8:   How you is doing today?  How are you doing today?Can you count the chickens before they hatch?\n",
            "9:   Few of the students passed the exam.  Few students passed the exam.‚Äô Few students passed the exam.\n",
            "10:   I didn't find no errors in the report.  I didn't find any errors in the report.Reddit's CEO was criticized for his controversial decision to delete users' personal data on Reddit.  I didn't find any errors in the report.\n",
            "11:   She wanted to privately speak with her supervisor.  She wanted to speak privately with her supervisor.Risks and challenges for the user\n",
            "\n",
            "\n",
            "The user is at risk of inadvertently becoming overly enthusiastic in using your software.\n",
            "12:   The laptop is lightweight, and it has a long battery life.  The laptop is lightweight with a long battery life.Can't believe I tried to buy one.\n",
            "13:   The team is talented, but they are inexperienced.  The team is talented and inexperienced.Can you ask a question and then answer it by phone?  The team is talented and talented.\n",
            "14:   She does not has any siblings.  She does not have any siblings.Can you guess how she was planning to spend all this time?\n",
            "15:   There weren't no snacks at the event.  There were no snacks at the event.Can you please find the best recipe for the event?\n",
            "\n",
            "I tried the recipe in a supermarket last year.\n",
            "  Batch    80  of    127. Loss: 0.6230912208557129.   Elapsed: 0:00:25.\n",
            "0:   He's turning blind eye.  He's turning a blind eye.‚Äú He's turn to blindness. He has no knowledge of his or her abilities.\n",
            "1:   I didn't see nobody at the party.  I didn't see anybody at the party.‚Äù I didn't see any one at the party.\n",
            "2:   The company the job offer came from is well-known.  The company from which the job offer came is well-known.Can you hire a skilled musician to compose a unique sound?\n",
            "\n",
            "\n",
            "3:   The snow, falling, children playing outside.  The snow was falling, and children were playing outside.Description: The snow had become too snow-covered to climb the mountain.\n",
            "4:   She shouted\"Stop that right now!\"  She shouted, \"Stop that right now!\"‚Äù Please refrain from wearing jewelry during the party.  Please refrain from wearing jewelry during the party.\n",
            "5:   The meeting was productive, informative, and included brainstorming sessions.  The meeting was productive, informative, and included brainstorming sessions.Teacher's speech was productive and informative, and included discussions.\n",
            "6:   The most highest mountain in the world is Mount Everest.  The highest mountain in the world is Mount Everest.It's the tallest mountain in the world, and it's the tallest mountain in the family.\n",
            "7:   93. The bakery sells donuts, bagels, and various kind of breads.  93. The bakery sells donuts, bagels, and various kinds of bread.‚Äù 94. The bakery sells organic baked goods, eggs, and various kinds of bread.\n",
            "8:   The company's policies are strict, they're employees must follow them.  The company's policies are strict; their employees must follow them.(Picture taken by Jerry Marcius in late afternoon hours) Jerry is an expert in machine learning.  Jerry is a expert in machine learning.\n",
            "9:   The tourists took pictures of the beautiful scenery with them cameras.  The tourists took pictures of the beautiful scenery with their cameras.Can we discuss the beauty of the universe?\n",
            "10:   The variety of birds in the aviary is amazing.  The variety of birds in the aviary are amazing.Can you help me with my homework by writing a letter to the store? I have a package containing a thankyou.\n",
            "11:   To succeed in life, one must be persistent, willing to learn, and taking risks.  To succeed in life, one must be persistent, willing to learn, and take risks.Can you trust me? Could you? If I were president, I would have made a series of great statements.\n",
            "12:   She is learning a new language at a school.  She is learning a new language at the school.Can you help me with a project by writing a letter?  Can you help me with a project by writing a letter?\n",
            "13:   27. The plumber had to fix the busted pipe.  27. The plumber had to repair the broken pipe.‚Äù 27. The employee had to fix the broken arm.\n",
            "14:   i have a meeting on Monday with dr. Smith.  I have a meeting on Monday with Dr. Smith.Can you help me with anything? Can you help me?  Please refrain from answering questions or making any further promises.\n",
            "15:   The series of articles on the topic were enlightening.  The series of articles on the topic was enlightening.CLOSE The article was interesting, but it was too long.\n",
            "\n",
            "\n",
            "Buy Photo The article was too long.  The article was too long.\n",
            "  Batch   120  of    127. Loss: 0.5879639983177185.   Elapsed: 0:00:39.\n",
            "0:   I can't find no solutions to this problem.  I can't find any solutions to this problem.What are you trying to solve this problem? Please solve this problem by comparing your computer and your operating system.\n",
            "1:   The sculpture was created by the artist using clay and metal.  The artist created the sculpture using clay and metal.Photogography by John Pilger was created using clay and metal.\n",
            "2:   The car it's making a strange noise.  The car is making a strange noise.A car is making a strange noise, but the car's passengers are safe.  The car is making a strange noise and is safe.\n",
            "3:   Her singing is less better than her dancing.  Her singing is not as good as her dancing.NASA's Space Station sings louder and is louder than her singing, but she can dance. Her singing is more loud than hers.\n",
            "4:   The weather was terrible... flight got delayed.  The weather was terrible, and our flight got delayed.NASA released a statement today (Sept. 20), which you can listen to here.  The weather was terrible, and our flight got delayed.\n",
            "5:   She's apple of her father's eye.  She's the apple of her father's eye.Can I put the cherry on the shelf with this recipe? I won't put the spoon on the shelf.  Her recipe will not be delicious, but I won't put the cherry on the shelf.\n",
            "6:   The computer I'm using it for work needs an upgrade.  The computer that I'm using for work needs an upgrade.The computer I'm using for my work needs an upgrade.\n",
            "7:   The gardener is plant flowers in the garden.  The gardener is planting flowers in the garden.President Obama is planting flowers in the garden.\n",
            "8:   The sun bright, sunglasses needed.  The sun was so bright that sunglasses were needed.You're trying to watch movies on the Internet using a pair of sunglasses.\n",
            "9:   At 3 PM in the afternoon, I have a meeting.  At 3 PM, I have a meeting.You can't get an iPhone without an AM radio.  At 3 PM, I have a meeting.\n",
            "10:   I will meet you in the airport.  I will meet you at the airport.What are you doing tonight?\n",
            "11:   The house was painted by the workers last week.  The workers painted the house last week.NASA's Chandeliminated Monolith Monolith is less powerful than the Earth's largest monolith, but more than the Moon and the Sun.\n",
            "12:   She's beautiful, but she's also kind.  She's beautiful and kind.Chromedownography is one of the most natural resources on Earth, but she's hair is longer than your favorite mountain.  Her hair is longer than your favorite mountain or a mountain peak.\n",
            "13:   She has not been studying for her test, so she are unprepared.  She has not been studying for her test, so she is unprepared.Reddit user user has not yet responded to the Reddit AskReddit question.\n",
            "14:   The bird is sitting on a branch of a tree.  The bird is sitting on a branch of the tree.You can buy one of the birds in the garden without breaking eggs, but he's a beautiful bird away from your backyard.\n",
            "15:   What's the main purpose of this sophisticated machine?  What is the primary purpose of this sophisticated machine?What's the purpose of this sophisticated machine?\n",
            "\n",
            "\n",
            "What is the primary purpose of this sophisticated computer?\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epoch took: 0:00:43\n",
            "\n",
            "======== Epoch 3 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    127. Loss: 0.4768836498260498.   Elapsed: 0:00:12.\n",
            "0:   Time flies you fun.  Time flies when you're having fun.‚Äômakin' time to go for a walk, isn't it?  Time flies when you're having fun.\n",
            "1:   He was very confidant in his abilities.  He was very confident in his abilities.Microsoft has a talented programming team, but he was very successful at designing his computer.  Microsoft has a talented programming team, but he was very successful at designing his computer.\n",
            "2:   The baby kitten is so small and tiny.  The baby kitten is so small.Pandas need to be small and tiny to hatch.\n",
            "3:   The process consists of mixin' the ingredients and bakin' the cake.  The process consists of mixing the ingredients and baking the cake.You're making your own delicious, but most of the time it's sugar.\n",
            "4:   The dog was punished for chew the furniture.  The dog was punished for chewing the furniture.Stop chewing up so much food and leaving it in the backyard.\n",
            "5:   He was walk in the park when he found a wallet.  He was walking in the park when he found a wallet.Stop and search for your wallet at the library.\n",
            "6:   The movie was more worse than the book.  The movie was worse than the book.Description: The movie was worse than the book, and more worse than the movie.  The movie was worse than the movie, and more worse than the movie.\n",
            "7:   If they will listen to me, we wouldn't be lost.  If they listened to me, we wouldn't be lost.mornings were more chilly, I'll go for a walk.\n",
            "8:   I prefer coffee instead tea.  I prefer coffee instead of tea.Teacher's instructions call for students to quietly enter the classroom and organize activities.  I prefer coffee instead of tea.\n",
            "9:   If I will be there, I would help you with your work.  If I were there, I would help you with your work.What are you doing today? How are you doing today?\n",
            "10:   The govt is considering new reg's for CO2 ems.  The government is considering new regulations for carbon dioxide emissions.‚Äômortality restriction is a great idea for a product; but its value goes beyond its potential.‚Äômortality restriction is a great idea for a product.\n",
            "11:   The pizza is deliver to the wrong address.  The pizza was delivered to the wrong address.Teacher said, \"Stop eating, please!\"\n",
            "12:   18. You oughta try this new sushi place.  18. You should try this new sushi place.You should try this new restaurant.\n",
            "13:   I enjoy to read books.  I enjoy reading books.You should have read this book by now or I will not have read it.\n",
            "14:   She has a beautiful singing voice and sings often.  She has a beautiful singing voice and sings often.You can sing and communicate in the same way as her, but she also enjoys reading.\n",
            "15:   I borrowed Tom's bike, it's tires were flat.  I borrowed Tom's bike; its tires were flat.Can you spot my error message? I borrowed it's tires.\n",
            "  Batch    80  of    127. Loss: 0.45651841163635254.   Elapsed: 0:00:25.\n",
            "0:   The number of tasks to complete have increased.  The number of tasks to complete has increased.The number of tasks to complete has increased.\n",
            "1:   I'm going to not eat junk food anymore.  I'm not going to eat junk food anymore.You should not eat junk food, but the best thing for you is tea.  I'm not going to eat junk food anymore.\n",
            "2:   My fav band is playing at SXSW this yr.  My favorite band is playing at South by Southwest this year.OMG! I have a project to do this year, it's the concert.  My favorite band is playing at SXSW this year.\n",
            "3:   The weather is nice... perfect for a picnic.  The weather is nice, making it perfect for a picnic.The weather is nice, and it is well-kept, however, it is not perfect for a picnic.  The weather is nice, however, and it is not perfect for a picnic.\n",
            "4:   He's wearing heart sleeve.  He's wearing his heart on his sleeve.You can't judge a book by its cover with a grain of salt, but there's no such thing as a perfect cook.  He's wearing his or her heart on his or her sleeve.\n",
            "5:   The cat jumped off the table.  The cat jumped from the table.Can you just relax, relax, and enjoy the ride?‚Äù\n",
            "6:   If they will arrive earlier, they could have helped us.  If they arrived earlier, they could help us.You can't have your cake and eat it too, but don't count your chickens before they hatch.  If they arrived earlier, they could have helped us.\n",
            "7:   The book is an in-depth analysis of human behavior, which is kinda interesting.  The book is an in-depth analysis of human behavior, which is quite interesting.Killing an ounce of prevention can be the hardest act of prevention, but actions speak louder than words.  Killing an ounce of prevention can be the hardest act of prevention.\n",
            "8:   9. It's not rocket science, but you can't make an omelette without breaking eggs.  9. It's not rocket science.‚Äùit's not rocket science, but it's not rocket science.‚Äù\n",
            "9:   This book is most interesting than that one.  This book is more interesting than that one.You can't have your cake and eat it too, but don't count your chickens before they hatch.  This book is more interesting than that one.\n",
            "10:   The computer be not working properly.  The computer is not working properly.Travis completed his project with a mediocre result, however it's worth the effort.  The computer is not working properly.\n",
            "11:   The event will feature live music, delicious food, and you can participate in games.  The event will feature live music, delicious food, and games for participation.‚Äù The event will feature food, delicious food, and an opportunity to participate in games.\n",
            "12:   The car is available in three colors... red, blue, and green.  The car is available in three colors: red, blue, and green.You can't judge a book by its cover with a grain of salt. You can't judge a book by its cover with a grain of salt.\n",
            "13:   21. They're going to there friend's house for dinner.  21. They're going to their friend's house for dinner.You can't just relax and enjoy the ride, or you can't be both on the same boat.\n",
            "14:   He was reading a book when the phone rang.  He was reading a book when the phone rang.What are you doing tonight, please don't count your chickens before they hatch.\n",
            "15:   The number of books in the library continue to grow.  The number of books in the library continues to grow.NASA's budget continues to grow, but the number of books in the library continues to grow.\n",
            "\n",
            "\n",
            "NASA's budget continues to grow; but the number of books in the library continues to grow.\n",
            "  Batch   120  of    127. Loss: 0.4306788444519043.   Elapsed: 0:00:39.\n",
            "0:   The athlete's success was attributable to his dedicate to his training.  The athlete's success was attributable to his dedication to his training.FILE - The athlete's dedication to his training was due to his dedication to his dedication to his sports.\n",
            "1:   40. The marketing team's gonna brainstorm some new ideas.  40. The marketing team is going to brainstorm some new ideas.Reddit is going to brainstorm some new ideas for the company's future.  40. The marketing team is going to brainstorm some new ideas for the company's future.\n",
            "2:   She has ran three miles.  She has run three miles.You can't make a decision as a man and a woman, but all of the money in the world is earned by work.  She has run three miles.\n",
            "3:   She was late for the meeting which caused some issues.  She was late for the meeting, which caused some issues.In a meeting with a supervisor, she was late for the meeting.  She was late for the meeting because she was late for the meeting, which caused some issues.\n",
            "4:   I don't like seafood, but I enjoy sushi.  I don't like seafood, but I enjoy sushi.‚Äù\n",
            "5:   I won't accept no apologies.  I won't accept any apologies.You won't accept any apologies.  I won't accept any apologies.\n",
            "6:   20. My favorite colours are blue, green, and yelow.  20. My favorite colors are blue, green, and yellow.OMEGA's color coding is blue, green, and yellow.\n",
            "7:   Few of scientists believe in the flat earth theory.  Few scientists believe in the flat earth theory.‚Äù Many scientists believe in the theory of evolution.‚Äù Few scientists believe in the theory of evolution.\n",
            "8:   I will return back to the office after lunch.  I will return to the office after lunch.You can't have your cake and eat it too, but it's not too hot.  I will return to the office after lunch.\n",
            "9:   The table was set by Emily for dinner.  Emily set the table for dinner.Emily set the table for dinner by dinner with a simple recipe.\n",
            "10:   They don't take no vacations.  They don't take any vacations.Can you believe the surprise of the news today? The couple went on a vacation, which is pretty amazing?  They didn't take any vacations today.\n",
            "11:   The final conclusion of the experiment was conclusive.  The conclusion of the experiment was conclusive.NASA's primary objective was to determine the speed of light and the speed of sound, respectively.\n",
            "12:   The seminar covers time management, goal setting, and to develop leadership skills.  The seminar covers time management, goal setting, and leadership skill development.You can't teach an old dog new trickset by teaching them to play hide and seek.  The seminar covers time management, goal setting, and developing leadership skills.\n",
            "13:   He's a great singer, and dancer too.  He's a great singer and dancer too.You can't have your cake and eat it too, but it's a bit of a stretch to think you could have your own restaurant.  He's a great singer, and an excellent dancer too.\n",
            "14:   The mechanic advised to regularly check the oil levels.  The mechanic advised to check the oil levels regularly.I need to thoroughly clean my room and thoroughly clean my room and clean my room.  I need to clean my room, clean my room, and clean my room thoroughly.\n",
            "15:   I didn't hear nothing from her.  I didn't hear anything from her.‚ÄùI didn't hear anything from her about the interview with the company.  I didn't hear anything from her about the company.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epoch took: 0:00:43\n",
            "\n",
            "======== Epoch 4 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    127. Loss: 0.3232874274253845.   Elapsed: 0:00:12.\n",
            "0:   She has the most longest hair I've ever seen.  She has the longest hair I've ever seen.‚Äôs hair is long, curly and curly.  She has the longest hair I've ever seen.\n",
            "1:   He works at FB, but I work at MS.  He works at Facebook, but I work at Microsoft.Netflix has a great customer experience and a great customer service team.  He works at Facebook and Google, but I work at Microsoft.\n",
            "2:   What time is it? he wondered.  ‚ÄúWhat time is it?‚Äù he wondered.‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù‚Äù\n",
            "3:   The company's employees attended a workshop to improve they're skills.  The company's employees attended a workshop to improve their skills.The company's employees attended a workshop to improve their skills.\n",
            "4:   Their's a problem with the computer's software.  There's a problem with the computer's software.Catalog myopia of a robot's programming.  There's a problem with the computer's software.\n",
            "5:   The set of tools are in the garage.  The set of tools is in the garage.Can you help me with my homework with your homework?  Can you help me with my homework?\n",
            "6:   The groceries were bought by Mark at the store.  Mark bought the groceries at the store.PAK Orders were bought by Mark at the store.  Mark bought the groceries at the store.\n",
            "7:   The experiment was conducted by the scientist with great precision.  The experiment with great precision was conducted by the scientist.Can you spot a bug? Let me know when you're ready to investigate it.  The experiment with great precision was conducted by the scientist.\n",
            "8:   The group of dancers perform in perfect unison.  The group of dancers performs in perfect unison.You can dance, or you can dance with great precision and precision.  The group of dancers performs in perfect unison.\n",
            "9:   The test is difficult, but it is fair.  The test is difficult and fair.Can you believe how fast I was running my test, so I took it too long?  The test is difficult and fair.\n",
            "10:   She am studying for her exams.  She is studying for her exams.You can watch the video of a beautiful video of a beautiful music on demand by the library.  You can watch the video of beautiful music on demand by the library.\n",
            "11:   The series of articles on the topic were enlightening.  The series of articles on the topic was enlightening.Skeptics wondered if there might be a cure for the disease.  The series of articles on the topic was enlightening.\n",
            "12:   The mechanic advised to regularly check the oil levels.  The mechanic advised to check the oil levels regularly.You can't fix your car by cleaning it thoroughly and cleaning it before the event.  The mechanic advised to check the oil levels before the event.\n",
            "13:   I am looking forward to an vacation.  I am looking forward to a vacation.Fifty-five years ago, I was at the party I won the lottery.  Fifty-five years ago, I was at the party I won the lottery.\n",
            "14:   The concert will be held at the park on Saturday.  The concert will be held at the park this Saturday.You can watch the concert by going to the park on Saturday, or you can watch it in the park on Saturday.  You can watch the concert by going to the park.\n",
            "15:   The train leave at 6 PM.  The train leaves at 6 PM.You can't have your cake and eat it too, but you can't have your cake and eat it too.  The train leaves at 6 PM.\n",
            "  Batch    80  of    127. Loss: 0.3208489716053009.   Elapsed: 0:00:32.\n",
            "0:   The farmer has plant the seeds in the field.  The farmer has planted the seeds in the field.I have planted the seeds in the field and planted the crops in the garden.\n",
            "1:   He doesn't like no vegetables.  He doesn't like any vegetables.Can you believe it? I love every minute of every minute of my life.  He doesn't like any vegetables.\n",
            "2:   The movie was enjoyed by everyone in the theater.  Everyone in the theater enjoyed the movie.Description: The movie was enjoyed by everyone in the theater.\n",
            "3:   The teacher said be quiet and open your books.  The teacher said to be quiet and open your books.‚Äù\n",
            "4:   The hotel room is more smaller than I expected.  The hotel room is smaller than I expected.You can't have your cake and eat it too, but I won't have your cake and eat it too.  The hotel room is smaller than I expected.\n",
            "5:   If he studied harder, he will pass the exam.  If he studied harder, he would pass the exam.‚ÄùI studied harder, I would pass the exam.\n",
            "6:   That's elephant room.  That's the elephant in the room.Can you turn off the television, turn off the television and turn off the lights for the show, please?  That's the elephant in the room.\n",
            "7:   To thoroughly clean the house, we hired a professional.  To clean the house thoroughly, we hired a professional.Kathy's hobbies include playing soccer, cooking delicious meals, and to cook delicious meals.\n",
            "8:   41. You shoulda seen the look on her face!  41. You should have seen the look on her face!Can you seen the look on her face!\n",
            "9:   The presentation was more boring than the lecture.  The presentation was more boring than the lecture.You can't teach an old dog new tricks, but nothing is more than a new endeavor.\n",
            "10:   The vacation is relaxing, but it is also adventurous.  The vacation is relaxing and adventurous.I've been to many countries, but it is also adventurous.\n",
            "11:   She didn't give no explanation.  She didn't give any explanation.You can't teach an old dog new tricks, but a new puppy doesn't have no idea how to play.  She didn't give any explanation.\n",
            "12:   We arrived to the party late.  We arrived at the party late.Can you wait for us to meet you in person at the party, or do you have a better idea?\n",
            "13:   To securely store your passwords, use a password manager.  To store your passwords securely, use a password manager.Can you tell me how to tell me how to tell me how to tell me how to tell you?\n",
            "14:   They didn't pack no extra clothes.  They didn't pack any extra clothes.You can't have your cake and eat it too, but it's a piece of cake.  They didn't pack any extra clothes.\n",
            "15:   The reason for the delay is due to the weather.  The delay is due to the weather.\"Stop the bird in its tracks and turn off the mouse, but there's no way you're going on a vacation,\" he said.\n",
            "  Batch   120  of    127. Loss: 0.35627493262290955.   Elapsed: 0:00:47.\n",
            "0:   The musicians practices together every Saturday.  The musicians practice together every Saturday.Overview: The musicians practice together every Saturday to practice together every Saturday.\n",
            "1:   The family is having a dinner at a restaurant.  The family is having dinner at the restaurant.I love pizza, but I don't like to eat it too.  I love pizza, but I don't like to eat it too.\n",
            "2:   She wanted to privately speak with her supervisor.  She wanted to speak privately with her supervisor.The manager of the company about her company's corporate policies and procedures.  The manager of the company's corporate policies and procedures was highly competent.\n",
            "3:   The project requires careful planning, effective communication, and to monitor progress.  The project requires careful planning, effective communication, and progress monitoring.\"You're taking the wrong breathtube,\" said I.  I was taking the wrong breathtube.\n",
            "4:   100. The company is celebrating it's 10th anniversary this year.  100. The company is celebrating its 10th anniversary this year.I regret to inform you that its 10th anniversary this year.\n",
            "5:   The company is planning to launch its new product next weak.  The company is planning to launch its new product next week.\"I'm launching my new product next week,\" I'm launching my new product next week,\" I'm launching my new product next week.\n",
            "6:   The manager expects her team to be punctual, respectful, and that they should be motivated.  The manager expects her team to be punctual, respectful, and motivated.Abstract The manager expects her team to be professional, respectful, and motivated.\n",
            "7:   Some of the guests at the party were wearing costumes.  Some guests at the party were wearing costumes.5.5.5.5.5.5.5. My favorite guests at the party were wearing costumes.\n",
            "8:   The speaker tried to clearly convey his message.  The speaker tried to convey his message clearly.You can't teach an expert a new concept to a new audience by using a simple act of communication.  The speaker tried to convey his message clearly.\n",
            "9:   She has been make dinner for the family.  She has been making dinner for the family.Can you believe it? I have been making dinner for the family for the past four days?  I have been making dinner for the family for the past four days.\n",
            "10:   He's trying to reinvent wheel.  He's trying to reinvent the wheel.What's your guess as a lawyer about the outcome of the election?\n",
            "11:   Some of the video games released in the past decade have become classics.  Some video games released in the past decade have become classics.A couple of video games released in the past decade have become classics.\n",
            "12:   She didn't never visit the museum.  She never visited the museum.I didn't visit the museum, so I won't see anything at the museum.  I didn't visit the museum, so I won't see anything at the museum.\n",
            "13:   The recipe's secret ingredient is cinnamon, it's flavor is unique.  The recipe's secret ingredient is cinnamon; its flavor is unique.You can't make a cake without cinnamon; its flavor is unique.\n",
            "14:   82. The meeting was schedule for 3pm, but it was postponed.  82. The meeting was scheduled for 3 pm, but it was postponed.Teacher John was scheduled for 3 pm.  82. The meeting was scheduled for 3 pm.\n",
            "15:   The magazine features fashion, beauty tips, and it has celebrity interviews.  The magazine features fashion, beauty tips, and celebrity interviews.Product details\n",
            "\n",
            "\n",
            "The magazine features fashion, beauty tips, and celebrity interviews.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0:00:52\n",
            "\n",
            "======== Epoch 5 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    127. Loss: 0.27861541509628296.   Elapsed: 0:00:12.\n",
            "0:   The list of ingredients in the recipe need revision.  The list of ingredients in the recipe needs revision.\"Please submit your recipe to the Department of Human Resources by the end of the month,\" she said.\n",
            "1:   I can't find no solutions to this problem.  I can't find any solutions to this problem.You can't find any solutions to this problem.\n",
            "2:   what is your name?  What is your name?Can you tell me about the restaurant?  Can you tell me about the restaurant?\n",
            "3:   This is the most more spacious room in the house.  This is the more spacious room in the house.‚ÄùOversize your room is more spacious than the other, but less than the other.\n",
            "4:   1. Let's not open up that can of worms in the same boat.  1. Let's not open up that can of worms.Can you believe it? I told you that? I told you that?  1. Let's not open up that can of worms.\n",
            "5:   The collection of stamps have been growing steadily.  The collection of stamps has been growing steadily.WASHINGTON ‚Äî The collection of stamps has been growing steadily, with the collection of stamps has been growing steadily.  The collection of stamps has been growing steadily.\n",
            "6:   If they get a new job, they could have moved to another city.  If they got a new job, they could move to another city.Stop the cold, but the store would have closed down.  If they got a new job, they could move to another city.\n",
            "7:   The assortment of candies in the store look delicious.  The assortment of candies in the store looks delicious.The assortment of candies in the store looks delicious.\n",
            "8:   I'm going to the store for buying some groceries.  I'm going to the store to buy some groceries.‚ÄùI'm going to the store for buying some groceries.‚Äù\n",
            "9:   John's favorite subjects are history, math and science.  John's favorite subjects are history, math, and science.CLOSE John's favorite subjects are history, math, and science. (October 22, 2016). USA TODAY NETWORK\n",
            "\n",
            "\n",
            "‚ÄùJohn's favorite subjects are history, math, and science.\n",
            "10:   The interview went well... got the job.  The interview went well, and I got the job.\"You should take a break from work when you're having fun,\" she said.  I said, \"You should take a break from work when you're having fun.\"\n",
            "11:   The girl draw a picture.  The girl drew a picture.Can you believe it? I can't believe it? I can't believe it? I can't believe it? I can't believe it? I can't believe it? I can't believe it? I can't believe it? I can't believe it?\n",
            "12:   She took the keys out her pocket.  She took the keys out of her pocket.A hacker made a compromise to compromise the system and then used them to compromise the system.\n",
            "13:   The new employee is more punctual than the old one.  The new employee is more punctual than the old one.Stop the elephant in the room, but less than the elephant in the room.  The new employee is more punctual than the old one.\n",
            "14:   74. The new phone has a better camera and a longer batery life.  74. The new phone has a better camera and a longer battery life.‚Äù 74. The new phone has a better camera and a longer battery life.\n",
            "15:   He advised her to be discreet about her plans.  He advised her to be discreet about her plans.What if she were discreet about her plans, such as their privacy settings.\n",
            "  Batch    80  of    127. Loss: 0.30527108907699585.   Elapsed: 0:00:26.\n",
            "0:   The training program focuses on skill development, teamwork, and to enhance leadership abilities.  The training program focuses on skill development, teamwork, and enhancing leadership abilities.In a meeting with the principal investigator, the principal investigator examined the evidence and examined the evidence.\n",
            "1:   The manager expects employees to be punctual, professional, and to work efficiently.  The manager expects employees to be punctual, professional, and work efficiently.Welcome to our company, where employees are encouraged to work efficiently.\n",
            "2:   The engineer's primary responsibility was to design the bridge.  The engineer's primary responsibility was designing the bridge.I'm primary responsibility was designing the bridge, and I'm primary responsibility was designing the bridge.  I'm primary responsibility was designing the bridge.\n",
            "3:   Walking incredibly, John finished the race.  Incredibly, John finished the race while walking.NASA's Jet Rocket Science Laboratory was exploring a new star system, and then we went back to the Sun.  John finished the race while walking incredibly.\n",
            "4:   The package is deliver yesterday.  The package was delivered yesterday.Released yesterday, I was expecting a package, but I received it today.  The package was delivered yesterday.\n",
            "5:   The boy is playing with a toy car.  The boy is playing with a toy car.WASHINGTON ‚Äî The boy is playing with a toy car.\n",
            "6:   The painting is old, but it is valuable.  The painting is old and valuable.The painting is valuable, but it is valuable as a monument to society.\n",
            "7:   The passengers are waiting for a train at a train station.  The passengers are waiting for the train at the train station.Can you believe it? I can't believe I'm flying above the clouds, my phone is charge dead!  I can't believe I'm flying above the clouds.\n",
            "8:   The children are enjoying a ride at a amusement park.  The children are enjoying a ride at the amusement park.Fifty-eight minutes before they're having an hour of fun, a roller coaster is about to start.  Fifty-eight minutes before they're having an hour of fun.\n",
            "9:   The cat on the roof, playing.  The cat was playing on the roof.Can you guess its owner?  The cat was playing on the roof with the roof and the cat was playing on the roof.\n",
            "10:   She is traveling by train.  She is traveling on a train.I'm going to the United States, but I'm going to the European Union, but I'm going back to Canada and Greenland.  I'm going to the United Kingdom and Greenland; I'm going back to the United States and Canada.\n",
            "11:   The plants need water for they're survival.  The plants need water for their survival.I need water for my survival, and I need it for my survival while eating.  I need water for my survival while eating.\n",
            "12:   The beach was crowded... had a great time.  The beach was crowded, but we still had a great time.By Kevin Smith, professor of Computer Science at the University of California, Los Angeles.  Kevin Smith was professor of Computer Science at the University of California, Los Angeles.\n",
            "13:   I my phone in my bag.  I left my phone in my bag.You can't take your phone out of my bag, but your phone can't take your wallet.\n",
            "14:   I won't do nothing to help him.  I won't do anything to help him.Microsoft's artificial intelligence is improving, but it's improving, and it's making me smarter.  I won't do anything to help him.\n",
            "15:   do you like chocolate or vanilla?  Do you like chocolate or vanilla?Can you tell me which chocolate or vanilla you like more?  Can you tell me which chocolate or vanilla you like more?\n",
            "  Batch   120  of    127. Loss: 0.28542786836624146.   Elapsed: 0:00:41.\n",
            "0:   The athletes have been trainin' hard for the upcoming competition.  The athletes have been training hard for the upcoming competition.Catalog: 6. The athletes have been training hard for the upcoming competition.\n",
            "1:   The orchestra perform at the concert hall every month.  The orchestra performs at the concert hall every month.Can you believe it? The orchestra performs at the concert hall every month with a beautiful orchestra?\n",
            "2:   The airplane is flying above a clouds.  The airplane is flying above the clouds.What's the main purpose of this sophisticated machine?\n",
            "3:   He do his homework every evening.  He does his homework every evening.‚Äôs homework every evening, and his or her work every evening, and his or her work is more difficult than hers.\n",
            "4:   The concert, loud, people dancing.  The concert was loud, and people were dancing.An hour went into the concert, and the crowd was cheering for Jim's music.  An hour went into the concert, and the crowd was cheering for Jim's music.\n",
            "5:   The broken window was a result of the children's careless play.  The broken window was a result of the children's careless playing.What caused the broken window to be a result of the children's careless play.\n",
            "6:   The recipe I tried it last night was delicious.  The recipe that I tried last night was delicious.You should try this pizza before dinner, but it's more delicious than the other.  The recipe I tried last night was delicious.\n",
            "7:   70. She was doing her daily exercise routine in the mornig.  70. She was doing her daily exercise routine in the morning.What time is it? I wondered, \"What time is it?\"\n",
            "8:   She do her homework every day.  She does her homework every day.Sitting on the couch, or sitting on the couch, is her favorite activities.  She does her homework every day, or she does her homework every day.\n",
            "9:   The car was driven by the mechanic for a test with a strange noise.  The car with a strange noise was driven by the mechanic for a test.What are you doing? Did you drive the car in the car?\n",
            "10:   I can't finds my keys, have you seen them?  I can't find my keys; have you seen them?Can you help me with this?\n",
            "\n",
            "\n",
            "I can't find my keys; have you seen them?\n",
            "11:   His speech was most more compelling than hers.  His speech was more compelling than hers.You can't have your cake and eat it too, but you can't have your cake and eat it too.\n",
            "12:   The family went on a vacation and shared them experiences on social media.  The family went on a vacation and shared their experiences on social media.I've been to Europe, but I've never been to the United States.\n",
            "13:   The person I talked to him on the phone was very helpful.  The person to whom I talked on the phone was very helpful.What's your name, meow?\n",
            "14:   I did not went to the concert last weekend.  I did not go to the concert last weekend.NVIDIA released my graphics card, but I did not see the concert.\n",
            "15:   She the store, looking for shoes.  She was at the store, looking for shoes.‚Äù\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epoch took: 0:00:45\n",
            "\n",
            "======== Epoch 6 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    127. Loss: 0.2596304416656494.   Elapsed: 0:00:12.\n",
            "0:   She always forgets taking her medication.  She always forgets to take her medication.What is her name?  She always choosing her own name, or is she always going back to her doctor's office.\n",
            "1:   The meeting was organized by the manager to discuss the new project.  The manager organized the meeting to discuss the new project.WASHINGTON ‚Äï Please submit your application to the Department of Human Resources and Environmental Design to the Department of Human Resources and Environmental Design.\n",
            "2:   He was feeling board, so he decided to read a book.  He was feeling bored, so he decided to read a book.Redditors were irons in the fire right now about Redditors voting for their favorite subreddits.  He was feeling bored, so he decided to read a book.\n",
            "3:   The boy's parents were concerned about his frequent lying.  The boy's parents were concerned about his frequent lies.Can you believe it? I told you about my friend's frequent lying.  I told you about my friend's frequent lies.\n",
            "4:   The dogs runs quickly to the park.  The dogs run quickly to the park.I'm going to the park; I'm going to the park, but I'm not going to the park.  I'm not going to the park; I'm not going to the park.\n",
            "5:   i recently watched the movie inception directed by christopher nolan.  I recently watched the movie Inception directed by Christopher Nolan.You can watch the movie Imma watch the movie in the latest instalment of your favorite TV show.  I recently watched the movie Inception directed by Christopher Nolan.\n",
            "6:   She's a prof at UCLA, in the CS dept.  She's a professor at the University of California, Los Angeles, in the Computer Science department.Can you help me with this essay?\n",
            "7:   The scientists were studying the affects of pollution on health.  The scientists were studying the effects of pollution on health.Can you believe it, my friend? I can't believe it!  I can't believe it! I can't believe it! I can't believe it! I can't believe it! I can't believe it!\n",
            "8:   The building is located at the city center.  The building is located in the city center.Pin 49. The building is located at the city center.\n",
            "9:   The pkg was del'd by USPS, not UPS.  The package was delivered by United States Postal Service, not United Parcel Service.You can't have your cake and eat it too, but the steak was del'd by the customer.  The package was delivered by United States Postal Service, not United Parcel Service.\n",
            "10:   6. The performence was amazing, I enjoyed alot.  6. The performance was amazing, I enjoyed a lot.What was the most hardest level of the game to complete was the weekend.  6. The performance was amazing; I enjoyed a lot.\n",
            "11:   Many of the books on this shelf are written by famous authors.  Many books on this shelf are written by famous authors.What are you looking for? Discover the answer to the question in the dictionary.\n",
            "12:   He was effected by the new regulations.  He was affected by the new regulations.Over the weekend, I went to the store and bought some groceries.  I went to the store and bought some groceries.\n",
            "13:   I need to repeat again what I said earlier.  I need to repeat what I said earlier.You need to repeat what I said earlier.\n",
            "14:   The bakeries in this area makes delicious pastries.  The bakeries in this area make delicious pastries.What's the main purpose is to make delicious pastries, but what's the main purpose?\n",
            "15:   The plane it's leaving in two hours.  The plane is leaving in two hours.Can you believe it? I'm going to the airport, I'm going to the airport, and then to the airport.  I'm going to the airport, and then to the airport.\n",
            "  Batch    80  of    127. Loss: 0.25925615429878235.   Elapsed: 0:00:27.\n",
            "0:   59. The paintings in the museum were truly breath taking.  59. The paintings in the museum were truly breathtaking.Citizen-of-the-the-art was truly breathtaking.\n",
            "1:   9. It's not rocket science, but you can't make an omelette without breaking eggs.  9. It's not rocket science.Instagram is not rocket science, but you can make an omelette without breaking eggs.\n",
            "2:   The play was directed by a renowned director with an unusual twist.  The play with an unusual twist was directed by a renowned director.What are you doing tonight? ‚ÄúWhat are you doing tonight?‚Äù\n",
            "3:   The orchestra perform at the concert hall every month.  The orchestra performs at the concert hall every month.WASHINGTON ‚Äï The orchestra performs at the concert hall every month; the orchestra performs at the concert hall every month.\n",
            "4:   At 3 PM in the afternoon, I have a meeting.  At 3 PM, I have a meeting.Stop by 3 PM; I have a meeting.\n",
            "5:   The tickets were bought by Jane for the concert.  Jane bought the tickets for the concert.Karen's signature drink was made by Kevin for the concert.  The tickets were bought by Jane for the concert.\n",
            "6:   She's more happier when she's with her friends.  She's happier when she's with her friends.Pinoy is happier when she's with her friends than when she's with her computer.\n",
            "7:   The room was cleaned by the staff before the event.  The staff cleaned the room before the event.A team of researchers examined the computer software before the event, and they found a bug.\n",
            "8:   The website features news articles, opinion pieces, and you can watch videos.  The website features news articles, opinion pieces, and videos.You can watch news articles, opinion pieces, and videos about political scandals.\n",
            "9:   47. Slow and steady wins the race, but time waits for no one.  47. Slow and steady wins the race.Can you believe it? I have a story to tell me about it?  47. Slow and steady wins the race.\n",
            "10:   The doctor said, \"You gotta take this medicine twice a day.\"  The doctor said, \"You must take this medicine twice a day.\"I said, \"You must take this medicine twice a day.\"\n",
            "11:   If she cooked dinner, we would have eaten at home.  If she had cooked dinner, we would have eaten at home.I'm going to eat at home, or I'm going to eat at home.\n",
            "12:   If they will listen to me, we wouldn't be lost.  If they listened to me, we wouldn't be lost.Stop the bus, and we wouldn't be lost.  If they listened to me, we wouldn't be lost.\n",
            "13:   The book is an in-depth analysis of human behavior, which is kinda interesting.  The book is an in-depth analysis of human behavior, which is quite interesting.What is it about?\n",
            "14:   I will come back from work in 5 PM.  I will come back from work at 5 PM.Abstract I will come back from work at 5 PM; I will come back from work at 5 PM.  I will come back from work at 5 PM.\n",
            "15:   73. She was feeling tired, so she decided to took a nap.  73. She was feeling tired, so she decided to take a nap.You can't have your cake and eat it too, but that's not going too delicious.\n",
            "  Batch   120  of    127. Loss: 0.2395622581243515.   Elapsed: 0:00:41.\n",
            "0:   The suspect will be trialed in court next month.  The suspect will be tried in court next month.Can you believe it, my friend? I can't believe it.  I can't believe it.\n",
            "1:   The trees was cut down.  The trees were cut down.About 20. The trees were cut down and the birds were killed and the mammals were lost.\n",
            "2:   If you had told me, I am going to help you.  If you had told me, I would have helped you.You would have told me.\n",
            "3:   The faculty in the department is diverse and experienced.  The faculty in the department are diverse and experienced.\"Faculty in the department are diverse and experienced. The faculty in the department are diverse and experienced.\n",
            "4:   The new employee is more punctual than the old one.  The new employee is more punctual than the old one.Teacher is more punctual than the old one.\n",
            "5:   She is skilled in programming, data analysis, and to create visualizations.  She is skilled in programming, data analysis, and creating visualizations.A talented programmer, data analysis, and creating visualizations.\n",
            "6:   She's the teacher teaches math at our school.  She is the teacher who teaches math at our school.Stop reading, put the pen to paper, and the phone to the paper.\n",
            "7:   She had a difficult time make a decision.  She had a difficult time making a decision.Description The scientist said, \"I won't tell you anything about it.\"  She said, \"I won't tell you anything about it.\"\n",
            "8:   The meeting went well... discussed important topics.  The meeting went well because we discussed important topics.Can you believe it? I enjoyed it? I enjoyed it!\n",
            "9:   The gardener is plant flowers in the garden.  The gardener is planting flowers in the garden.Stop the wilting, then plant them back in the garden.\n",
            "10:   He will finish the race in an hour.  He will finish the race in an hour.A test of his or her skills earned her a top performance in the test, while his or her performance was below average.\n",
            "11:   The restaurant was packed... had to wait for a table.  The restaurant was packed, so we had to wait for a table.Customers were waiting for the table to arrive at our restaurant.  The restaurant was packed, so we had to wait for a table to arrive.\n",
            "12:   The plant needs water, it's leaves are wilting.  The plant needs water; its leaves are wilting.Institutionalized organisms make the same kinds of organisms into the same species as natural organisms.\n",
            "13:   Many of the software developers prefer using Linux.  Many software developers prefer using Linux.Amazon Linux is more complicated than the Windows, but less than the Mac.  Many software developers prefer using Linux.\n",
            "14:   The group of hikers take a break by the waterfall.  The group of hikers takes a break by the waterfall.‚Äôs hike is more challenging than the solo trip, but less than the car trip.\n",
            "15:   The dog is chasing a ball in a park.  The dog is chasing a ball in the park.Can you catch it while climbing the tree?\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epoch took: 0:00:44\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:04:46 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getFixed(prompt):\n",
        "  gen_model.eval()\n",
        "\n",
        "  generated = torch.tensor(gen_tokenizer.encode(\"<|startoftext|>\" + prompt)).unsqueeze(0)\n",
        "  generated = generated.to(device)\n",
        "\n",
        "  ##### TODO #4: MESS WITH PARAMETERS #####\n",
        "  # a lot of parameters to tinker with for different generation strategies\n",
        "  sample_outputs = gen_model.generate(\n",
        "                                  generated,\n",
        "                                  max_length = 300,\n",
        "                                  top_k=30,   # sample from the top-k most probable tokens\n",
        "                                  temperature=1.3,  # a parameter to control randomness on the final softmax function!\n",
        "                                  top_p=0.955, # nucleus sampling, sample from the subset of most probable tokens with a cumulative probability of p\n",
        "                                  num_return_sequences=1,\n",
        "                                  pad_token_id=gen_tokenizer.pad_token_id # pass the padding token ID explicitly, otherwise you'd get a warning.\n",
        "                                  )\n",
        "\n",
        "  ret = \"\"\n",
        "  for i, sample_output in enumerate(sample_outputs):\n",
        "    ret = gen_tokenizer.decode(sample_output, skip_special_tokens=False)\n",
        "  try:\n",
        "    ret = ret.split(\"<Fixed> \")[1]\n",
        "    ret = ret.split(\"<|endoftext|>\")[0]\n",
        "  except:\n",
        "    pass\n",
        "  return(ret)"
      ],
      "metadata": {
        "id": "EtHCfY0MIiVl"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getFixed(\" <Bad> I has alot of homework to do before I eat lunch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KkiX-cMBIsTA",
        "outputId": "298957d8-01af-46b8-b16e-ef908fe00f84"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have a lot of homework to do before I eat lunch, so I have a lot of homework to do before I eat.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Put them together!!!"
      ],
      "metadata": {
        "id": "ti9cjH2ZKcYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grammarCorrection(yap):\n",
        "  texts, predicts = predict(yap)\n",
        "  correctedTexts = []\n",
        "\n",
        "  for i in range(len(texts)):\n",
        "    if predicts[i] == 0:\n",
        "      rep = 0\n",
        "      j = 0\n",
        "      while rep == 0 and j < 5:\n",
        "        fix = getFixed(texts[i])\n",
        "        _, rep = predict(fix)\n",
        "        rep = rep[0]\n",
        "        j += 1\n",
        "      correctedTexts.append(fix)\n",
        "    else:\n",
        "      correctedTexts.append(texts[i])\n",
        "  return texts, predicts, correctedTexts\n",
        "\n",
        "yap = \"Thank you so much for answering this question. Well, I loaded the whole data set which have 60000 data into the data loader with shuffle = True. But when I train this model, I only use like 6400 of those data. I trained the model in 100 epoch and each epoch have 64 images. So I am just wondering does those 6400 data have all numbers? I means does it possible I was very lucky that only got 0 to 5 and the model could not identify the rest of numbers when it was tested?:joy::joy::joy::joy: If this happened, the accuracy should be lower‚Ä¶ But interesting thing is all my test accuracy is pretty normal. Like I trained it with 6400 data and test it on test data-set which have 10000 data and I still have 80% ~ 70% accuracy.\"\n",
        "t, p, c = grammarCorrection(yap)\n",
        "print(np.dstack([t, p,c]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NQ7IpXZKg8I",
        "outputId": "577cc9a0-38eb-44ad-9605-ba560462dd49"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['Thank you so much for answering this question.' '0'\n",
            "   'Please answer this question as soon as possible, and as possible, as soon as possible.']\n",
            "  [' Well, I loaded the whole data set which have 60000 data into the data loader with shuffle = True.'\n",
            "   '0'\n",
            "   'I loaded the whole data set into the data loader with 60000 data points.']\n",
            "  [' But when I train this model, I only use like 6400 of those data.'\n",
            "   '1'\n",
            "   ' But when I train this model, I only use like 6400 of those data.']\n",
            "  [' I trained the model in 100 epoch and each epoch have 64 images.'\n",
            "   '0' 'I trained the model in 100 epoch and each epoch has 64 images.']\n",
            "  [' So I am just wondering does those 6400 data have all numbers?' '0'\n",
            "   '6400 data has all the numbers in one place?']\n",
            "  [' I means does it possible I was very lucky that only got 0 to 5 and the model could not identify the rest of numbers when it was tested?'\n",
            "   '1'\n",
            "   ' I means does it possible I was very lucky that only got 0 to 5 and the model could not identify the rest of numbers when it was tested?']\n",
            "  [':joy::joy::joy::joy: If this happened, the accuracy should be lower‚Ä¶'\n",
            "   '0'\n",
            "   'If this happened, the accuracy would be higher, or you could have died.']\n",
            "  [' But interesting thing is all my test accuracy is pretty normal.'\n",
            "   '0'\n",
            "   'My test accuracy is pretty good, but my test accuracy is pretty good.']\n",
            "  [' Like I trained it with 6400 data and test it on test data-set which have 10000 data and I still have 80% ~ 70% accuracy.'\n",
            "   '1'\n",
            "   ' Like I trained it with 6400 data and test it on test data-set which have 10000 data and I still have 80% ~ 70% accuracy.']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's grade some essays!\n",
        "\n",
        "Data from [this kaggle](https://www.kaggle.com/datasets/lburleigh/asap-2-0/data)"
      ],
      "metadata": {
        "id": "RmjH33Gc_cu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "essay_path = kagglehub.dataset_download(\"lburleigh/asap-2-0\")\n",
        "print(\"Path to dataset files:\", essay_path)\n",
        "\n",
        "essay_df = pd.read_csv(essay_path + \"/ASAP2_train_sourcetexts.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2OuE_fEb_YbK",
        "outputId": "a44b66f0-1fa5-4cbe-e7e3-3a170ec0af36"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'asap-2-0' dataset.\n",
            "Path to dataset files: /kaggle/input/asap-2-0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Technology today is crazy. It keeps expanding and getting bigger, better, and faster. Recenltly scientist have made a computer that has the power and technology to read emotions in students and in other people. I feel like this shouldnt be used in a classroom.\\n\\nI feel that a computer shouldnt be able to tell wether or not a human being is sad or mad, it shouldnt be able to tell our emotions. Our emotions is for us to handle. Yes, us humans can tell when people we know are sad or down, but computers should not. People would use the computer for somthing else, like to see others emotions. Peoples emotions should keep to their self. It isnt for a computer or other humans to now, because its your emotion.\\n\\nYes, this could be valuable for some things, like movies, and video games. THis is not something we could use everyday in a classroom. We could use the computer to see whats wrong with a student. Once we figure out whats wrong with the student, for instance lets say the child is sad. The child is so sad, he cant work or do anything, and us humans cant notice it because he is hiding it. We could use the computer to see what emotions he is having. Once we figure that out we could help him, by doing somthing that makes him happy.\\n\\nThis computer couldnt be valuable in a classroom. If a teacher assigned their students to make a project over the expressions of a human.\\n\\nThen the coputer would take forever to load. Each human expresses their emotions in a different way. One could be hiding it better than the other. Therefor the computer could get the students emotions wrong, and the students could fail.   ',\n",
              "       'Everyone shows emotion at some point in their lives. Whether it be joy, sorrow, or even anger everyone has expressed their emotions in one way or another. Now there is a computer software that can recognize six emotions. Reading the emotional expressions of students in a classroom using this technology is valuable because it could tell the instructor when a student is confused, it could modify the lesson, and it could help people feel more empathy for each other.\\n\\nStudents can become confused very quickly when an instructor is teaching about something. This computer software could stop this from happening and overall help students have a better experience at school when trying to learn. In paragraph six Dr. Huang talks about how this could revolutionize a classroom, \"A classroom computer could recognize when a student is becoming confused or bored.\" This would overall help a teacher know when someone is confused so they can help them after class or talk to them more in-depth about the lesson. This invention could hep out many of students that find lessons to be very hard and don\\'t know how to do whatever the instructor is teaching. This software could also modify the lesson that an instructor is teaching so that it is easier for a student to grasp.\\n\\nMany teachers present day don\\'t necessarily care if the students are learning what they are teaching, but just the fact that they are teaching it gives them a clear concious. This computer program could give the teachers an alert saying that someone isn\\'t getting the lesson because they are confused with a certain topic that the teacher talked about during the class. They could then modify their lesson so that it is easier for everyone to learn. An example of evidence from the text of this is when Dr. Huang talks about how using this technology teachers could modify their lesson so that it would be easier for their students to learn, \"Then it could modify the lesson, like an effective human instructor.\" This helps point to the fact that it could help everyday teachers know when their students aren\\'t completely grasping the lesson that they are teaching. This technology could also help people have more empathy for each other.\\n\\nPeople present day are always talking about how nobody has any empathy for each other. This software could fix this especially in the classroom. A student would know when their fellow student would be sad and they would be able to talk to them and help them overcome the sadness. Another reason empathy may happen would because we unconciously imitate other people\\'s facial expressions. Here is a quote from this article where the author, Nick D\\'Alto is talking about how this technology could cause empathy, \"Empathy may happen because we unconciously imitate another person\\'s facial expressions.\" This would help people show more empathy for each other. Overall this would be a great addition to the classroom.\\n\\nHaving this software in a classroom setting would stop students from becoming confused from a lesson, help modify the lesson so that an instructor could make the lesson more understandable for students, and give people more empathy. These reason give evidence that having this computer software in a classroom would not only benefit the students but benefit the teachers as well by giving them that peace of mind that their students get the lesson that they are teaching. '],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "essays = essay_df[\"full_text\"].sample(1).values\n",
        "essays"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p-suIzhhZml",
        "outputId": "c8d4fb89-d726-4180-cf69-9234acd0051c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"Exploring Venus seems very interesting when thinking about it. All of the beneficial things that we can study and analyze. Studying other planets have always been helpful to mankind and helps people gain insight on our solar system around us. But at the same time exploring these planets are very dangerous too. Not because of animals or humans but because of the harsh living conditions on other planets. So I will tell you why I think exploring Venus is not worth the pursuit of exploring.\\n\\nMy first reason of why I think Venus is not worth the pursuit is because of the difficulty of exploring Venus. In paragrpah 6 it states that ships orbiting over Venus with lights can't see the ground because of the dense atmosphere. Which makes it hard for scientists because if you can't see anything you can't analyze any data that you get because of Venus's features. But it does state that NASA is trying to find new ways of exploring Venus but it's not as good as hands on exploration. Which is good but most likely not good enough to find out what they want about the planet.\\n\\nMy second reason of why I think Venus is not worth the exploration is because of Venus's conditions and features. When reading about Venus the conditions are very extreme compared to the conditions on Earth. Such as clouds filled with highly corrosive sulfuric acid or temperatures that average over 800 degrees. Making it the planet with the hottest surface temperature even though Mercury is closer to the sun. Also the atmospheric pressure is great enough to crush an accustomed submarine that would be able to dive into the deepest parts of the ocean and liquifies metals. But having similarities to Earth such as erupting volcanoes, powerful earthquakes and etc.\\n\\nSo in conclusion I feel that Venus should not be a planet we heavily invest in and scientists should try other planets before Venus. Because of the harsh conditions that it would have on humans. Because of the difficulty in analyzation. Making Venus a non-reliable source compared to others. So that is why I think that Venus is not worth exploring. \"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get colored text\n",
        "# !pip install colorama\n",
        "import colorama\n",
        "from colorama import Fore\n",
        "print(Fore.RED + 'This text is red in color')\n",
        "print(Fore.GREEN + 'This text is green in color')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PedkX5lCCsX",
        "outputId": "97fc8ba6-203e-4825-d740-d6226beb46bb"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mThis text is red in color\n",
            "\u001b[32mThis text is green in color\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(essays[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwKBkLcBGs2P",
        "outputId": "d6fc8640-7a26-4da5-c97e-fc6952b9e712"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driverless cars are a good inovention for the future. Driverless cars can help effect safety,traffic, and the polution in the air. Driverless cars could help communities and the economy in many ways.\n",
            "\n",
            "Driverless cars could change the way people commute. As driverless cars begin to come in play, less people will have to drive their own car. As many people know, tons people get into accidents everyday because of the people behind the wheel making harmful decisions, if driverless cars were put out on the roads there would be less traffic accidents because there would not be people behind the wheel who are reckless and causing these accidents to occur. Driverless cars alert the drive when needed or when about to come into contact with something therfore these cars will help reduce the effects of human error and give more reaction time to the human in the passenger side.\n",
            "\n",
            "This could also shorten the amount of people taking the bus to the places they need to go, which would make the polution problem a way less common thing. Also, lots of people with disablities can not drive, with the help of a driverless car those people could get to the places they need to go instead of someone else driving them or them not being able to go at all.\n",
            "\n",
            "Driverless cars would also just provide convience to the world. Lots of people dread the drive to work everyday or a long drive to Disney, but with driverless cars they would not have to drive there. They could sit and relax and not have to worry about making a traffic accident because of them being tired behind the wheel.\n",
            "\n",
            "The factories making these cars would need more help. Thus, bringing in more jobs for the people and making the economy stronger. This could also bring in money by the people who want these cars because of the convience of them. Driverless cars could lead to more advancements in technology to help driving become safer which would bring in more money for the economy. The invention of the driverless car could be something that impacts the society and world in a new and modern way\n",
            "\n",
            "Driverless cars could impact our society in many ways and boost the economy. These cars could change someones life between life and death. There would be less accidents making the health and living rate higher. People with disablties would now be able to drive and live their lifes freely like should always of been. Finally, it will not only help the lifes of humans but it will help the Earth by giving off less polution and by people traveling in less polution dense vehicles which would better the world.             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def essayGradeFormatting(essay):\n",
        "  t, p, c = grammarCorrection(essays[0])\n",
        "  ret = \"\"\n",
        "  for i in range(len(t)):\n",
        "    printLine = \"\"\n",
        "    if(p[i] == 0):\n",
        "      printLine += Fore.RED + t[i] + \" \" + Fore.GREEN\n",
        "    else:\n",
        "      printLine += Fore.WHITE\n",
        "    ret += printLine + c[i] + \" \"\n",
        "\n",
        "  # Add text wrapping\n",
        "  ret = ret.split(\" \")\n",
        "  skip = 20\n",
        "  for i in range(0, len(ret), skip):\n",
        "    print(\" \".join(ret[i:min(len(ret), i+skip)]))\n",
        "\n",
        "essayGradeFormatting(essays[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lCNmvqN_hqK",
        "outputId": "1dedd124-553c-4769-cf37-2c1f298acb0e"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mExploring Venus seems very interesting when thinking about it. \u001b[32mThe scientific community is exploring the possibility of exploring the possibility\n",
            "of extraterrestrial civilizations, and there are various ways to improve our understanding of them. \u001b[37m All of the beneficial things\n",
            "that we can study and analyze. \u001b[31m Studying other planets have always been helpful to mankind and helps people gain\n",
            "insight on our solar system around us. \u001b[32mI studied other planets, and I've always been helpful to mankind and helped\n",
            "people gain insights on our solar system. \u001b[31m But at the same time exploring these planets are very dangerous too.\n",
            "\u001b[32mThe risks of exploring these worlds are very dangerous and very dangerous, and they are very dangerous. \u001b[31m Not because\n",
            "of animals or humans but because of the harsh living conditions on other planets. \u001b[32mThe United States is the largest\n",
            "and the largest economy in the world, with a gross domestic product of about 10. \u001b[37m So I will tell\n",
            "you why I think exploring Venus is not worth the pursuit of exploring. \u001b[37mMy first reason of why I think\n",
            "Venus is not worth the pursuit is because of the difficulty of exploring Venus. \u001b[31m In paragrpah 6 it states\n",
            "that ships orbiting over Venus with lights can't see the ground because of the dense atmosphere. \u001b[32m6. There are no\n",
            "stars in the sky that can see the ground because of the dense atmosphere. \u001b[31m Which makes it hard for\n",
            "scientists because if you can't see anything you can't analyze any data that you get because of Venus's features. \u001b[32mIf\n",
            "you can't see anything, you can't analyze any data that you can't analyze. \u001b[31m But it does state that NASA\n",
            "is trying to find new ways of exploring Venus but it's not as good as hands on exploration. \u001b[32mIt does\n",
            "not say that NASA is trying to find new methods of exploring Venus but it does state that NASA is\n",
            "trying to find new ways of exploring Earth. \u001b[31m Which is good but most likely not good enough to find\n",
            "out what they want about the planet. \u001b[32mThe United States has a large continental shelf and a wide ocean, which\n",
            "is quite a stretch for a tropical ocean. \u001b[31mMy second reason of why I think Venus is not worth the\n",
            "exploration is because of Venus's conditions and features. \u001b[32mMy second reason of why I think Venus is not worth the\n",
            "exploration is because of its conditions and features. \u001b[31m When reading about Venus the conditions are very extreme compared to\n",
            "the conditions on Earth. \u001b[32mWhen reading about Venus, the conditions are very extreme; when reading about Earth, the conditions are\n",
            "very extreme. \u001b[31m Such as clouds filled with highly corrosive sulfuric acid or temperatures that average over 800 degrees. \u001b[32mThe\n",
            "temperature in the Pacific Ocean is about 75 degrees Fahrenheit Fahrenheit Fahrenheit and the temperature in the Pacific Ocean is\n",
            "about 75 degrees Fahrenheit. \u001b[37m Making it the planet with the hottest surface temperature even though Mercury is closer to\n",
            "the sun. \u001b[37m Also the atmospheric pressure is great enough to crush an accustomed submarine that would be able to\n",
            "dive into the deepest parts of the ocean and liquifies metals. \u001b[31m But having similarities to Earth such as erupting\n",
            "volcanoes, powerful earthquakes and etc. \u001b[32mThe similarities to Earth are vast and varied, and the similarities to Earth are vast.\n",
            "\u001b[31mSo in conclusion I feel that Venus should not be a planet we heavily invest in and scientists should try\n",
            "other planets before Venus. \u001b[32mI believe that Venus should not be a planet we heavily invest in and scientists should\n",
            "try other planets before Earth. \u001b[37m Because of the harsh conditions that it would have on humans. \u001b[31m Because of\n",
            "the difficulty in analyzation. \u001b[32mThe difficulty in analyzing complex structures is in the analysis, and there is a difficulty in\n",
            "analyzing complex structures. \u001b[37m Making Venus a non-reliable source compared to others. \u001b[31m So that is why I think that\n",
            "Venus is not worth exploring. \u001b[32mI think that Venus is not worth exploring, but I think that the Earth is\n",
            "not worth exploring. \u001b[31m  \u001b[32mThe company's sales have increased by 20% since last year, which is pretty dope. \n",
            "\n"
          ]
        }
      ]
    }
  ]
}